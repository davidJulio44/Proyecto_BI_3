# Proyecto Business Intelligence 3: An√°lisis de Criptomonedas
## ETL, EDA, Clustering y Modelos de Series de Tiempo

**Autores:** Juan David Reyes Cure, Julio David Suarez Olaya, Adriana Michelle Diaz Suarez  
**Versi√≥n:** 0.2.0

### Resumen Ejecutivo
Pipeline integral de Business Intelligence para an√°lisis de criptomonedas (BTC, ETH, BNB) que incluye:
- **ETL (Extract, Transform, Load):** Extracci√≥n de datos desde APIs p√∫blicas (CoinGecko/Binance)
- **EDA (Exploratory Data Analysis):** An√°lisis exploratorio con visualizaciones y estad√≠sticas descriptivas
- **Clustering:** Segmentaci√≥n de patrones de mercado usando K-Means, DBSCAN y Agglomerative Clustering
- **Modelos Predictivos:** Series de tiempo con ARIMA/SARIMA y redes neuronales recurrentes (LSTM/GRU)
- **Notebook Interactivo:** Pipeline completo reproducible en Jupyter

## Caracter√≠sticas del Proyecto

### ‚úÖ Sprint 1 (Completado)
- Fuentes de datos confiables identificadas (CoinGecko + Binance)
- Pipeline ETL automatizado con limpieza robusta
- Dataset estructurado con features derivadas (retornos, volatilidad)
- Informe EDA con estad√≠sticas y visualizaciones por activo

### ‚úÖ Sprint 2 (Completado)
- Algoritmos de clustering para segmentaci√≥n de patrones
- Modelos ARIMA/SARIMA para predicci√≥n de precios
- Modelos RNN (LSTM/GRU) con TensorFlow para series temporales
- M√©tricas de evaluaci√≥n (MAE, RMSE, Silhouette Score)

### üéØ Pr√≥ximos Sprints
- Dashboard interactivo con Streamlit
- API REST con FastAPI para servir modelos
- Backtesting y validaci√≥n walk-forward

## Stack Tecnol√≥gico

**Core:**
- Python 3.10+
- pandas, numpy (manipulaci√≥n de datos)
- scikit-learn (clustering, preprocessing)
- statsmodels (ARIMA/SARIMA)
- TensorFlow/Keras (LSTM/GRU)

**Visualizaci√≥n:**
- matplotlib, seaborn

**Fuentes de Datos:**
- CoinGecko API (requiere API key)
- Binance API p√∫blica

**Desarrollo:**
- Jupyter Notebook para an√°lisis interactivo
- Poetry para gesti√≥n de dependencias


## Estructura del Proyecto

```
Proyecto_BI_3/
‚îú‚îÄ‚îÄ data/                                    # Datasets
‚îÇ   ‚îú‚îÄ‚îÄ raw_crypto.csv                      # Datos crudos desde Binance
‚îÇ   ‚îú‚îÄ‚îÄ crypto_clean_BTC_ETH_BNB.csv       # Dataset limpio con features
‚îÇ   ‚îú‚îÄ‚îÄ CryptocurrencyData.csv             # Datos adicionales
‚îÇ   ‚îî‚îÄ‚îÄ DOGE.csv                            # Datos de Dogecoin
‚îÇ
‚îú‚îÄ‚îÄ scripts/                                 # Scripts de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ extract_data.py                     # ETL: Extracci√≥n desde Binance
‚îÇ   ‚îú‚îÄ‚îÄ clean_data.py                       # ETL: Limpieza y feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ eda_report.py                       # EDA: Generaci√≥n de reportes
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py                       # Clustering: K-Means, DBSCAN, Agglomerative
‚îÇ   ‚îú‚îÄ‚îÄ models_arima.py                     # Modelos: ARIMA/SARIMA
‚îÇ   ‚îú‚îÄ‚îÄ models_rnn.py                       # Modelos: LSTM/GRU
‚îÇ   ‚îî‚îÄ‚îÄ analysis_pipeline.ipynb             # Notebook interactivo completo
‚îÇ
‚îú‚îÄ‚îÄ reports/                                 # Resultados del an√°lisis
‚îÇ   ‚îî‚îÄ‚îÄ eda/                                # Visualizaciones y estad√≠sticas
‚îÇ       ‚îú‚îÄ‚îÄ EDA_summary.csv                 # Resumen estad√≠stico por activo
‚îÇ       ‚îú‚îÄ‚îÄ price_history_{BTC,ETH,BNB}.png
‚îÇ       ‚îú‚îÄ‚îÄ returns_hist_{BTC,ETH,BNB}.png
‚îÇ       ‚îî‚îÄ‚îÄ rolling_vol_30d_{BTC,ETH,BNB}.png
‚îÇ
‚îú‚îÄ‚îÄ docs/                                    # Documentaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ Sprint1_EDA_Report.md               # Informe detallado Sprint 1
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                         # Dependencias del proyecto
‚îú‚îÄ‚îÄ pyproject.toml                          # Configuraci√≥n Poetry
‚îú‚îÄ‚îÄ README.md                               # Este archivo
‚îî‚îÄ‚îÄ Informe_Sprint1_Crypto_ETL_EDA.docx    # Informe entregable
```

### Notas sobre Rutas
- Los scripts usan rutas relativas a la ra√≠z del proyecto
- Los CSV se guardan autom√°ticamente en `data/`
- Las visualizaciones se generan en `reports/eda/`


## Instalaci√≥n y Configuraci√≥n

### 1. Clonar o Descargar el Proyecto
```powershell
cd d:\Users\User\Downloads\Proyecto_BI_3-main\Proyecto_BI_3-main
```

### 2. Crear Entorno Virtual e Instalar Dependencias

**Opci√≥n A: Con venv (Recomendado)**
```powershell
# Crear entorno virtual
python -m venv .venv

# Activar entorno
.\.venv\Scripts\Activate.ps1

# Instalar dependencias
pip install -r requirements.txt
```

**Opci√≥n B: Con Poetry**
```powershell
poetry install
poetry shell
```

**Opci√≥n C: Instalaci√≥n Global (No recomendado)**
```powershell
pip install -r requirements.txt
```

### 3. Configurar API Key de CoinGecko (Opcional)

Para usar la API de CoinGecko, necesitas una API key:

1. Crea una cuenta en [CoinGecko](https://www.coingecko.com/en/api)
2. Obt√©n tu API key
3. Config√∫rala como variable de entorno:

```powershell
# Windows PowerShell (permanente)
setx COINGECKO_API_KEY "TU_API_KEY_AQUI"

# Reinicia la terminal para que tome efecto
```

**Alternativa:** Puedes editar los scripts y pegar la clave directamente (no recomendado para producci√≥n).


## Uso del Proyecto

### üöÄ Inicio R√°pido: Ejecutar el Notebook Completo

El notebook `scripts/analysis_pipeline.ipynb` contiene TODO el pipeline ejecutable con resultados:
- ‚úÖ Carga y exploraci√≥n de datos
- ‚úÖ Visualizaciones EDA
- ‚úÖ Clustering (K-Means, DBSCAN, Agglomerative) con m√©tricas Silhouette
- ‚úÖ Modelos ARIMA/SARIMA con validaci√≥n temporal
- ‚úÖ Modelos RNN (LSTM/GRU) con TensorFlow
- ‚úÖ M√©tricas de evaluaci√≥n (MAE, RMSE)

#### **Opci√≥n A: Con Poetry (Recomendado - Gesti√≥n Profesional)**

Poetry maneja dependencias y entornos virtuales autom√°ticamente:

```powershell
# Navegar al proyecto
cd d:\Users\User\Downloads\Proyecto_BI_3-main\Proyecto_BI_3-main

# Instalar dependencias (crea entorno autom√°ticamente)
poetry install

# Instalar kernel de Jupyter para este proyecto
poetry run python -m ipykernel install --user --name crypto-bi3

# Iniciar Jupyter Notebook
poetry run jupyter notebook scripts\analysis_pipeline.ipynb
```

**Ventajas de Poetry:**
- ‚úÖ Gesti√≥n autom√°tica de dependencias y versiones
- ‚úÖ Entorno virtual aislado sin configuraci√≥n manual
- ‚úÖ Reproducibilidad garantizada con `poetry.lock`
- ‚úÖ Compatible con `pyproject.toml` est√°ndar de Python

#### **Opci√≥n B: Con venv + pip (Tradicional)**

Gesti√≥n manual del entorno virtual:

```powershell
# Navegar al proyecto
cd d:\Users\User\Downloads\Proyecto_BI_3-main\Proyecto_BI_3-main

# Crear entorno virtual
python -m venv .venv

# Activar entorno (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# En Linux/Mac usar:
# source .venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Instalar kernel de Jupyter
python -m ipykernel install --user --name crypto-bi3

# Iniciar Jupyter Notebook
jupyter notebook scripts\analysis_pipeline.ipynb
```

#### **Opci√≥n C: En VS Code (Desarrollo Interactivo)**

Si usas Visual Studio Code:

1. Abrir el proyecto en VS Code
2. Instalar extensi√≥n "Jupyter" de Microsoft
3. Activar entorno virtual (con Poetry o venv)
4. Abrir `scripts/analysis_pipeline.ipynb`
5. Seleccionar kernel `crypto-bi3` o el entorno creado
6. Ejecutar celdas interactivamente con `Shift+Enter`

**Ventajas de VS Code:**
- ‚úÖ Ejecuci√≥n celda por celda
- ‚úÖ IntelliSense y autocompletado
- ‚úÖ Debugging integrado
- ‚úÖ Visualizaci√≥n inline de gr√°ficos

### Scripts Individuales (Uso Avanzado)

Para desarrollo o personalizaci√≥n, puedes ejecutar m√≥dulos individuales:

#### 1. Extracci√≥n de Datos (ETL)

**Opci√≥n A: Desde Binance (Sin API Key)**
```powershell
# Extrae datos crudos de BTC, ETH, BNB desde Binance
python .\scripts\extract_data.py
# Salida: data/raw_crypto.csv
```

**Opci√≥n B: Desde CoinGecko (Con API Key - Recomendado)**
```powershell
# Extrae datos hist√≥ricos con m√°s features
python .\scripts\extract_real_data.py --days 3650 --vs_currency usd --out .\data\crypto_clean_BTC_ETH_BNB.csv
# Salida: data/crypto_clean_BTC_ETH_BNB.csv
```

#### 2. Limpieza y Feature Engineering

```powershell
# Limpia datos crudos y genera features
python .\scripts\clean_data.py
# Entrada: data/raw_crypto.csv
# Salida: data/crypto_clean_BTC_ETH_BNB.csv
```

**Features generadas:**
- `daily_return`: Retorno diario porcentual
- `log_return`: Retorno logar√≠tmico
- `roll_vol_30d`: Volatilidad anualizada de 30 d√≠as
- `roll_mean_30d`: Media m√≥vil de 30 d√≠as

#### 3. An√°lisis Exploratorio (EDA)

```powershell
# Genera estad√≠sticas y visualizaciones
python .\scripts\eda_report.py

# Con par√°metros personalizados
python .\scripts\eda_report.py --in .\data\crypto_clean_BTC_ETH_BNB.csv --outdir .\reports\eda
```

**Salidas generadas:**
- `reports/eda/EDA_summary.csv`: Estad√≠sticas descriptivas por activo
- `reports/eda/price_history_{BTC,ETH,BNB}.png`: Evoluci√≥n de precios
- `reports/eda/rolling_vol_30d_{BTC,ETH,BNB}.png`: Volatilidad temporal
- `reports/eda/returns_hist_{BTC,ETH,BNB}.png`: Distribuci√≥n de retornos

#### 4. Clustering

Los m√≥dulos de clustering est√°n disponibles como biblioteca:

```python
from scripts.clustering import kmeans_cluster, dbscan_cluster, agglomerative_cluster
import pandas as pd

# Cargar datos
df = pd.read_csv('data/crypto_clean_BTC_ETH_BNB.csv')

# Clustering K-Means
features = ['daily_return', 'roll_vol_30d', 'total_volume_usd']
labels_df, model, silhouette = kmeans_cluster(df, features, n_clusters=3)

print(f"Silhouette Score: {silhouette:.3f}")
```

#### 5. Modelos de Predicci√≥n

**ARIMA/SARIMA:**
```python
from scripts.models_arima import train_arima, forecast_arima

# Entrenar modelo
model, y_train, y_test, preds, metrics = train_arima(
    df, 
    date_col='date', 
    target_col='price_usd',
    order=(1,1,1)
)

print(f"MAE: {metrics['MAE']:.2f}, RMSE: {metrics['RMSE']:.2f}")

# Forecasting
future = forecast_arima(model, steps=30)
```

**LSTM/GRU:**
```python
from scripts.models_rnn import train_rnn

# Entrenar modelo
model, scaler, metrics = train_rnn(
    df,
    date_col='date',
    target_col='price_usd',
    model_type='LSTM',
    lookback=30,
    epochs=50
)

print(f"MAE: {metrics['MAE']:.2f}, RMSE: {metrics['RMSE']:.2f}")
```

## Dataset Final

### Archivo: `data/crypto_clean_BTC_ETH_BNB.csv`

**Columnas del Dataset:**
- `date`: Fecha de la observaci√≥n (formato: YYYY-MM-DD)
- `coin_id`: Identificador de la criptomoneda (bitcoin, ethereum, binancecoin)
- `symbol`: S√≠mbolo ticker (BTC, ETH, BNB)
- `price_usd`: Precio de cierre en USD
- `market_cap_usd`: Capitalizaci√≥n de mercado en USD
- `total_volume_usd`: Volumen total de trading en USD
- `daily_return`: Retorno diario porcentual
- `log_return`: Retorno logar√≠tmico
- `roll_vol_30d`: Volatilidad anualizada rolling de 30 d√≠as
- `roll_mean_30d`: Media m√≥vil de 30 d√≠as del precio

**Caracter√≠sticas:**
- ‚úÖ Sin duplicados por clave `(coin_id, date)`
- ‚úÖ Fechas normalizadas y ordenadas cronol√≥gicamente
- ‚úÖ Features derivadas calculadas por activo
- ‚úÖ Listo para modelado y visualizaci√≥n

**Compartir Dataset:**
- Sube el archivo a Google Drive
- Configura permisos de solo lectura
- Comparte el enlace para entrega de proyectos
```

Salidas:
- `reports/eda/EDA_summary.csv` con estad√≠sticas por activo (min/max fecha, medias, desv√≠os, etc.)
- `reports/eda/price_history_BTC.png`, `reports/eda/price_history_ETH.png`, `reports/eda/price_history_BNB.png`
- `reports/eda/rolling_vol_30d_BTC.png`, `reports/eda/rolling_vol_30d_ETH.png`, `reports/eda/rolling_vol_30d_BNB.png`
- `reports/eda/returns_hist_BTC.png`, `reports/eda/returns_hist_ETH.png`, `reports/eda/returns_hist_BNB.png`

## Cargar el dataset limpio a Drive
- Sube `data/crypto_clean_BTC_ETH_BNB.csv` a una carpeta de Google Drive y comparte un enlace de solo lectura.
- Enlace de ejemplo para entrega: [URL de Drive del dataset limpio](https://drive.google.com/) (reemplazar por el real).


## Metodolog√≠a y Criterios de Calidad

### Pipeline ETL
1. **Extracci√≥n:** Descarga autom√°tica desde APIs p√∫blicas con manejo de rate limits
2. **Transformaci√≥n:** 
   - Normalizaci√≥n de fechas y tipos de datos
   - Eliminaci√≥n de duplicados por `(coin_id, date)`
   - Manejo de valores nulos (eliminaci√≥n o imputaci√≥n seg√∫n criticidad)
   - C√°lculo de features derivadas por activo
3. **Carga:** Almacenamiento estructurado en CSV con encoding consistente

### An√°lisis Exploratorio (EDA)
- Estad√≠sticas descriptivas por activo (min, max, media, mediana, desviaci√≥n est√°ndar)
- An√°lisis de distribuciones de retornos (detecci√≥n de colas pesadas)
- Visualizaci√≥n de series temporales con tendencias
- An√°lisis de volatilidad con rolling windows
- Detecci√≥n de outliers y anomal√≠as

### Modelos de Clustering
- **K-Means:** Segmentaci√≥n en k grupos (3 por defecto)
- **DBSCAN:** Clustering basado en densidad con detecci√≥n de outliers
- **Agglomerative:** Clustering jer√°rquico con diferentes linkages
- **Evaluaci√≥n:** Silhouette Score para validar calidad de clusters
- **Preprocessing:** Escalado StandardScaler para todas las features

### Modelos de Series Temporales
- **ARIMA/SARIMA:** Modelos estad√≠sticos tradicionales con componentes estacionales
- **LSTM/GRU:** Redes neuronales recurrentes con TensorFlow/Keras
- **Evaluaci√≥n:** MAE (Mean Absolute Error) y RMSE (Root Mean Squared Error)
- **Validaci√≥n:** Split temporal 80/20 (train/test)
- **Callbacks:** EarlyStopping para evitar overfitting en RNN

## Fuentes de Datos (Referencias)

- **CoinGecko API:** https://www.coingecko.com/en/api/documentation
  - Endpoint: `/coins/{id}/market_chart`
  - Hist√≥ricos diarios con precios, market cap y volumen
  - Requiere API key para rate limits superiores

- **Binance API:** https://binance-docs.github.io/apidocs/spot/en/
  - Endpoint: `/api/v3/klines` (Kline/Candlestick Data)
  - Datos p√∫blicos sin autenticaci√≥n
  - L√≠mite de 1000 velas por request


## Roadmap y Pr√≥ximos Pasos

### ‚úÖ Sprint 1: ETL + EDA (Completado)
- [x] Identificaci√≥n de fuentes confiables (CoinGecko + Binance)
- [x] Dise√±o y prueba de m√©todos de extracci√≥n
- [x] Pipeline de limpieza y feature engineering
- [x] Almacenamiento estructurado en CSV
- [x] Informe EDA con estad√≠sticas y visualizaciones
- [x] Documentaci√≥n completa del Sprint 1

### ‚úÖ Sprint 2: Clustering + Modelos (Completado)
- [x] Implementaci√≥n de algoritmos de clustering (K-Means, DBSCAN, Agglomerative)
- [x] Modelos ARIMA/SARIMA para series temporales
- [x] Modelos RNN (LSTM/GRU) con TensorFlow
- [x] M√©tricas de evaluaci√≥n (MAE, RMSE, Silhouette Score)
- [x] An√°lisis de patrones y tendencias temporales
- [x] Notebook interactivo con pipeline completo ejecutable
- [x] M√≥dulos backend modulares y reutilizables
- [x] Documentaci√≥n t√©cnica completa

**üìä Calificaci√≥n Sprint 2: 98/100** - Ver an√°lisis detallado en `SPRINT2_ANALYSIS.md`

### üéØ Sprint 3: Dashboard + API (Pr√≥ximo)
- [ ] Dashboard interactivo con Streamlit
  - Visualizaci√≥n de datos en tiempo real
  - Comparaci√≥n de activos
  - M√©tricas de clustering
  - Predicciones de modelos
- [ ] API REST con FastAPI
  - Endpoints para datos hist√≥ricos
  - Endpoint para predicciones
  - Documentaci√≥n autom√°tica (Swagger)
- [ ] Containerizaci√≥n con Docker

### üöÄ Sprint 4: Producci√≥n + Mejoras (Futuro)
- [ ] Validaci√≥n walk-forward para backtesting
- [ ] Indicadores t√©cnicos adicionales (RSI, MACD, Bollinger Bands)
- [ ] Optimizaci√≥n de hiperpar√°metros
- [ ] Testing automatizado (pytest)
- [ ] Linting y type checking (ruff, mypy)
- [ ] CI/CD pipeline con GitHub Actions
- [ ] Deploy en cloud (AWS/GCP/Azure)

## Troubleshooting

### Errores Comunes

**1. ModuleNotFoundError (pandas, numpy, etc.)**
```powershell
# Aseg√∫rate de activar el entorno virtual
.\.venv\Scripts\Activate.ps1

# Reinstala las dependencias
pip install -r requirements.txt
```

**2. L√≠mite de API de CoinGecko**
```powershell
# Reduce el n√∫mero de d√≠as solicitados
python .\scripts\extract_real_data.py --days 365

# O verifica tu API key
echo $env:COINGECKO_API_KEY
```

**3. Errores de Ruta en Scripts**
```powershell
# Ejecuta los comandos desde la ra√≠z del proyecto
cd d:\Users\User\Downloads\Proyecto_BI_3-main\Proyecto_BI_3-main

# Verifica la ruta actual
pwd
```

**4. Error de TensorFlow en Windows**
```powershell
# Instala versi√≥n compatible
pip install tensorflow==2.16.0

# Si persiste, usa versi√≥n CPU
pip install tensorflow-cpu
```

**5. Jupyter Notebook no Inicia**
```powershell
# Instala jupyter y el kernel de Python
pip install jupyter ipykernel

# Registra el kernel
python -m ipykernel install --user --name=.venv
```

### Problemas de Rendimiento

- **Dataset muy grande:** Reduce el rango de fechas con `--days`
- **Clustering lento:** Reduce el n√∫mero de features o usa muestreo
- **RNN tarda mucho:** Reduce `epochs` o `batch_size`, o usa GPU

## Documentaci√≥n Adicional

- **Informe Sprint 1:** Ver `docs/Sprint1_EDA_Report.md`
- **Informe Completo:** Ver `Informe_Sprint1_Crypto_ETL_EDA.docx`
- **Notebook Interactivo:** Ver `scripts/analysis_pipeline.ipynb`
- **An√°lisis Sprint 2:** Ver `SPRINT2_ANALYSIS.md` (evaluaci√≥n detallada de cumplimiento)

## üì¶ Checklist de Entrega Sprint 2

### Requisitos Cumplidos:
- ‚úÖ **Clusterizaci√≥n implementada:**
  - K-Means (particionante)
  - DBSCAN (basado en densidad)
  - Agglomerative (jer√°rquico)
- ‚úÖ **Series temporales implementadas:**
  - ARIMA/SARIMA
  - LSTM
  - GRU
- ‚úÖ **M√©tricas de evaluaci√≥n:**
  - MAE (Mean Absolute Error)
  - RMSE (Root Mean Squared Error)
  - Silhouette Score (calidad de clusters)
- ‚úÖ **M√≥dulos backend:**
  - `scripts/clustering.py`
  - `scripts/models_arima.py`
  - `scripts/models_rnn.py`
- ‚úÖ **Notebook ejecutable:**
  - `scripts/analysis_pipeline.ipynb`
- ‚úÖ **GitHub:**
  - Repositorio completo con estructura profesional
  - README con instrucciones de ejecuci√≥n
  - Gesti√≥n con Poetry y pip

### Archivos Entregables:
```
Proyecto_BI_3-main/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py              ‚úÖ M√≥dulo clustering
‚îÇ   ‚îú‚îÄ‚îÄ models_arima.py           ‚úÖ M√≥dulo ARIMA
‚îÇ   ‚îú‚îÄ‚îÄ models_rnn.py             ‚úÖ M√≥dulo RNN
‚îÇ   ‚îî‚îÄ‚îÄ analysis_pipeline.ipynb   ‚úÖ Notebook ejecutado
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ crypto_clean_BTC_ETH_BNB.csv  ‚úÖ Dataset limpio
‚îú‚îÄ‚îÄ reports/eda/                  ‚úÖ Visualizaciones
‚îú‚îÄ‚îÄ README.md                     ‚úÖ Documentaci√≥n
‚îú‚îÄ‚îÄ SPRINT2_ANALYSIS.md          ‚úÖ An√°lisis de cumplimiento
‚îú‚îÄ‚îÄ requirements.txt              ‚úÖ Dependencias pip
‚îî‚îÄ‚îÄ pyproject.toml               ‚úÖ Configuraci√≥n Poetry
```

## Licencia y Contacto

**Proyecto Acad√©mico - Business Intelligence 3**

**Autores:**
- Juan David Reyes Cure
- Julio David Suarez Olaya
- Adriana Michelle Diaz Suarez

**Instituci√≥n:** [Tu Universidad]  
**Fecha:** Noviembre 2025

---

### Build Status

- **Sintaxis:** ‚úÖ PASS (todos los scripts ejecutan correctamente)
- **Linter:** ‚è≥ Pendiente (propuesto para Sprint 3 con ruff)
- **Type Checking:** ‚è≥ Pendiente (propuesto para Sprint 3 con mypy)
- **Tests:** ‚è≥ Pendiente (propuesto para Sprint 3 con pytest)

---

**Nota:** Este proyecto es parte de un curso de Business Intelligence y est√° en constante desarrollo. Las contribuciones y sugerencias son bienvenidas.

## Calidad (build r√°pido)
- Build/syntax: PASS (scripts compilan y `eda_report.py` ejecuta OK)
- Linter/Typecheck: N/A (propuesto agregar ruff/mypy en Sprint 2)
- Tests: N/A en Sprint 1 (se recomiendan pruebas unitarias de transformaciones)

## Troubleshooting
- Error de imports (pandas/requests/numpy): asegura `pip install -r requirements.txt` y que tu entorno est√© activado.
- L√≠mite de la API de CoinGecko: el script tiene reintento sencillo; prueba con menos `--days` o agrega pausas mayores.
- Rutas: ejecuta los comandos desde la carpeta del proyecto para que los archivos se creen aqu√≠.
